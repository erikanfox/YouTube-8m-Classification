{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, BatchNormalization, Dropout, Concatenate\n",
    "from keras.optimizers import adam_v2,gradient_descent_v2\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from time import time\n",
    "import multiprocess as mp\n",
    "import tensorflow as tf\n",
    "from multiprocessing import Pool\n",
    "from keras.layers import GRU,Bidirectional,LSTM,MaxPool1D,GlobalAveragePooling1D\n",
    "from keras.callbacks import EarlyStopping\n",
    "import eval_util as eval\n",
    "import mean_average_precision_calculator as map_calculator\n",
    "import average_precision_calculator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metrics \n",
    "def ap_at_n(data):\n",
    "    # based on https://github.com/google/youtube-8m/blob/master/average_precision_calculator.py\n",
    "    predictions, actuals = data\n",
    "    n = 20\n",
    "    total_num_positives = None\n",
    "\n",
    "    if len(predictions) != len(actuals):\n",
    "        raise ValueError(\"the shape of predictions and actuals does not match.\")\n",
    "\n",
    "    if n is not None:\n",
    "        if not isinstance(n, int) or n <= 0:\n",
    "            raise ValueError(\"n must be 'None' or a positive integer.\"\n",
    "                             \" It was '%s'.\" % n)\n",
    "\n",
    "    ap = 0.0\n",
    "\n",
    "    sortidx = np.argsort(predictions)[::-1]\n",
    "\n",
    "    if total_num_positives is None:\n",
    "        numpos = np.size(np.where(actuals > 0))\n",
    "    else:\n",
    "        numpos = total_num_positives\n",
    "\n",
    "    if numpos == 0:\n",
    "        return 0\n",
    "\n",
    "    if n is not None:\n",
    "        numpos = min(numpos, n)\n",
    "    delta_recall = 1.0 / numpos\n",
    "    poscount = 0.0\n",
    "\n",
    "    # calculate the ap\n",
    "    r = len(sortidx)\n",
    "    if n is not None:\n",
    "        r = min(r, n)\n",
    "    for i in range(r):\n",
    "        if actuals[sortidx[i]] > 0:\n",
    "            poscount += 1\n",
    "            ap += poscount / (i + 1) * delta_recall\n",
    "    return ap\n",
    "\n",
    "\n",
    "def gap(pred, actual):\n",
    "    lst = zip(list(pred), list(actual))\n",
    "    with mp.Pool() as pool:\n",
    "        all = pool.map(ap_at_n, lst)\n",
    "\n",
    "    return np.mean(all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_frame(frame_data,n_frame_sample):\n",
    "\n",
    "  rgb_by_vid = list(map(\n",
    "      lambda frames: \n",
    "      np.array(frames)[\n",
    "        np.random.choice(len(frames),size=n_frame_sample)\n",
    "        ],frame_data))\n",
    "\n",
    "  X= np.array(rgb_by_vid)\n",
    "  return(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER = '/Users/marlynehakizimana/Documents/SPRING/IDS705/FinalProj/frame/' #path to: train, val, test folders with *tfrecord files\n",
    "def process_records(data,tp='test'):\n",
    "    #print(batch)\n",
    "    tfiles = sorted(glob.glob(os.path.join(FOLDER, tp, '*tfrecord')))\n",
    "    # train=['train0093.tfrecord','train0111.tfrecord','train0208.tfrecord','train0274.tfrecord','train076.tfrecord']\n",
    "    # validation=['validate0052.tfrecord','validate0259.tfrecord','validate0267.tfrecord']\n",
    "    # test=['validate3731.tfrecord']\n",
    "    #print('total files in %s %d' % (tp, len(tfiles)))\n",
    "    #tfiles=[FOLDER+tp+\"/\"+i for i in data]\n",
    "    ids,aud,rgbs, lbs = [],[],[],[]\n",
    "    for fn in tfiles :\n",
    "        \n",
    "        for example in tf.data.TFRecordDataset(fn).take(500):#tf.python_io.tf_record_iterator(fn):\n",
    "            tf_example = tf.train.SequenceExample()#tf.train.Example.FromString(example)\n",
    "            rt=tf_example.ParseFromString(example.numpy())\n",
    "            yss = np.array(tf_example.context.feature[\"labels\"].int64_list.value)\n",
    "            out = np.zeros(2000).astype(np.int8) #number of classes 1000\n",
    "            rgb=[]\n",
    "            audio=[]\n",
    "            ty=len(tf_example.feature_lists.feature_list['rgb'].feature)\n",
    "            #print(\"long\",len(yss))\n",
    "            if np.sum([True for i in yss if i<=1000])==len(yss) and ty>200 :\n",
    "                for y in yss:\n",
    "                    out[y] = 1\n",
    "                for k in range(200):#np.random.randint(0,,100):\n",
    "                    rgb.append(np.array(tf.io.decode_raw(tf_example.feature_lists.feature_list['rgb'].feature[k].bytes_list.value[0],tf.uint8)))\n",
    "                    audio.append(np.array(tf.io.decode_raw(tf_example.feature_lists.feature_list['audio'].feature[k].bytes_list.value[0],tf.uint8)))\n",
    "                ids.append(tf_example.context.feature[\"id\"].bytes_list.value[0].decode(encoding=\"UTF-8\"))\n",
    "                lbs.append(out)\n",
    "                aud.append(audio)\n",
    "                rgbs.append(rgb)\n",
    "    \n",
    "       \n",
    "    return np.array(ids),np.array(aud), np.array(rgbs), np.array(lbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_block(x,n=4096):\n",
    "    x = Dense(n)(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x=GlobalAveragePooling1D(data_format='channels_last',keepdims=False)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    return x\n",
    "def build_mod():\n",
    "    #in1 = Input((50,128), name='x1')\n",
    "    #print(in1.shape)\n",
    "    #x1 = fc_block(in1)\n",
    "    #print(x1.shape)\n",
    "    in1 = Input((50,1024), name='x1')\n",
    "    x1 = fc_block(in1)\n",
    "    x=x1\n",
    "    #print(x2.shape)\n",
    "    #x = Concatenate(axis=-1)([x1, x2])\n",
    "    #print(x.shape)\n",
    "    #x = Dense(1024)(x)\n",
    "    #x = BatchNormalization()(x)\n",
    "    #x = LeakyReLU()(x)\n",
    "    #x = Dropout(0.2)(x)\n",
    "    #x=fc_block(x)\n",
    "    #print(x)\n",
    "    out = Dense(2000, activation='sigmoid', name='output')(x)\n",
    "    \n",
    "    model = Model(inputs=[in1], outputs=out)\n",
    "    #print(model.summary())\n",
    "    opt = adam_v2.Adam(learning_rate=0.0001)\n",
    "    #opt=gradient_descent_v2.SGD(learning_rate=0.001)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train,Y_train,X_val,Y_val):\n",
    "    #if not os.path.exists('weights'): os.mkdir('weights')\n",
    "    model = build_mod()\n",
    "    callback = EarlyStopping(monitor='loss', patience=3)\n",
    "    model.fit(X_train[1],Y_train,epochs=100,batch_size=50,\n",
    "                 validation_data = (X_val[1],Y_val),callbacks=[callback])\n",
    "    model.save_weights('weights.h5')\n",
    "    model.load_weights('weights.h5')\n",
    "    y_prd = model.predict({'x1': X_val[1]}, verbose=False, batch_size=50)\n",
    "    g = gap(y_prd, Y_val)\n",
    "    hit=eval.calculate_hit_at_one(y_prd, y_val)\n",
    "    print(\"Hit at one\",hit)\n",
    "    print(\"val GAP\",g)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_labels(data, tp='test'):\n",
    "    #print(batch)\n",
    "    #tfiles=[FOLDER+tp+\"/\"+i for i in data]\n",
    "    #print('total files in %s %d' % (tp, len(tfiles)))\n",
    "    tfiles = sorted(glob.glob(os.path.join(FOLDER, tp, '*tfrecord')))\n",
    "    ids,ys=[],[]\n",
    "    for fn in tfiles:\n",
    "        \n",
    "        for example in tf.data.TFRecordDataset(fn).take(500):\n",
    "            tf_example = tf.train.SequenceExample()\n",
    "            rt=tf_example.ParseFromString(example.numpy())\n",
    "            \n",
    "            yss = np.array(tf_example.context.feature[\"labels\"].int64_list.value)\n",
    "            ty=len(tf_example.feature_lists.feature_list['rgb'].feature)\n",
    "            if np.sum([True for i in yss if i<=1000])==len(yss)and ty>200:\n",
    "                ids.append(tf_example.context.feature[\"id\"].bytes_list.value[0].decode(encoding=\"UTF-8\"))\n",
    "                ys.append(yss)\n",
    "            \n",
    "    return np.array(ys,dtype=object), np.array(ids)\n",
    "def conv_pred(el): #gives the top 20 classes \n",
    "    t = 10\n",
    "    idx = np.argsort(el)[::-1]\n",
    "    return [el[i] for i in idx[:t]]#' '.join(['{} {:0.5f}'.format(i, el[i]) for i in idx[:t]])\n",
    "def another_pred(el): #gives the top 20 classes \n",
    "    t = 10\n",
    "    idx = np.argsort(el)[::-1]\n",
    "    return [i for i in idx[:t]]\n",
    "def predict(idx,X_test,Y_test,ylabels):\n",
    "    model = build_mod()\n",
    "    model.load_weights('weights.h5')\n",
    "    ypd = model.predict({'x1': X_test[1]}, verbose=1, batch_size=10)\n",
    "    g = gap(ypd, Y_test)\n",
    "    hit=eval.calculate_hit_at_one(ypd, Y_test)\n",
    "    print(\"Hit at one\",hit)\n",
    "    print(\"int\",g)\n",
    "    #del x1_val, x2_val\n",
    "    with mp.Pool() as pool:\n",
    "        out = pool.map(conv_pred, list(ypd))\n",
    "        out1 = pool.map(another_pred, list(ypd))\n",
    "    df = pd.DataFrame.from_dict({'VideoId': idx,\"True labels\":ylabels,\"Pred labels\":out1, 'Confidence': out})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5592, 200, 128)\n",
      "(9978, 200, 128)\n",
      "(2251, 200, 128)\n"
     ]
    }
   ],
   "source": [
    "train_data=['train0093.tfrecord','train0111.tfrecord','train0208.tfrecord','train0274.tfrecord','train0276.tfrecord']\n",
    "validation_data=['validate0052.tfrecord','validate0259.tfrecord','validate0267.tfrecord']\n",
    "test_data=['validate3731.tfrecord']\n",
    "\n",
    "#_, x1_val, x2_val, y_val = process_records(validation_data,'validation')\n",
    "_, x1_val, x2_val, y_val = process_records('validation','validation')\n",
    "print(x1_val.shape)\n",
    "#_, x1_train, x2_train, y_train=process_records(train_data,'train')\n",
    "_, x1_train, x2_train, y_train=process_records('train','train')\n",
    "print(x1_train.shape)\n",
    "#idx, x1_test, x2_test, y_test=process_records(test_data,'test')\n",
    "idx, x1_test, x2_test, y_test=process_records('test','test')\n",
    "print(x1_test.shape)\n",
    "#ylabels,ids=process_labels(test_data,'test')\n",
    "ylabels,ids=process_labels('test','test')\n",
    "\n",
    "x1_val=sample_frame(x1_val,50)\n",
    "x2_val=sample_frame(x2_val,50)\n",
    "x1_train=sample_frame(x1_train,50)\n",
    "x2_train=sample_frame(x2_train,50)\n",
    "x1_test=sample_frame(x1_test,50)\n",
    "x2_test=sample_frame(x2_test,50)\n",
    "X_train,Y_train=[x1_train,x2_train],y_train\n",
    "X_val,Y_val=[x1_val,x2_val],y_val\n",
    "idx,X_test,Y_test,ylabels=idx,[x1_test,x2_test],y_test,ylabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pooling RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "50/50 [==============================] - 72s 1s/step - loss: 19.9818 - val_loss: 24.8436\n",
      "Epoch 2/100\n",
      "50/50 [==============================] - 76s 2s/step - loss: 14.9344 - val_loss: 17.4164\n",
      "Epoch 3/100\n",
      "50/50 [==============================] - 70s 1s/step - loss: 11.6524 - val_loss: 15.1138\n",
      "Epoch 4/100\n",
      "50/50 [==============================] - 65s 1s/step - loss: 9.9360 - val_loss: 14.2380\n",
      "Epoch 5/100\n",
      "50/50 [==============================] - 63s 1s/step - loss: 9.4918 - val_loss: 14.2949\n",
      "Epoch 6/100\n",
      "50/50 [==============================] - 65s 1s/step - loss: 9.9338 - val_loss: 15.8687\n",
      "Epoch 7/100\n",
      "50/50 [==============================] - 64s 1s/step - loss: 11.1191 - val_loss: 18.5803\n",
      "Epoch 8/100\n",
      "50/50 [==============================] - 62s 1s/step - loss: 13.0051 - val_loss: 21.4933\n",
      "Hit at one 0.6045918367346939\n",
      "val GAP 0.4989032024114854\n"
     ]
    }
   ],
   "source": [
    "train(X_train,Y_train,X_val,Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 3s 161ms/step\n",
      "Hit at one 0.5067567567567568\n",
      "int 0.42127686131652853\n"
     ]
    }
   ],
   "source": [
    "df_pooling_rgb=predict(idx,X_test,Y_test,ylabels)\n",
    "#Hit@1=0.51\n",
    "#Gap=0.42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save csv\n",
    "#df_pooling_rgb.to_csv(\"data/Frame_level_Pooling_rgb.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POOLING AUDIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mod():\n",
    "    in1 = Input((50,128), name='x1')\n",
    "    #print(in1.shape)\n",
    "    x1 = fc_block(in1)\n",
    "    #print(x1.shape)\n",
    "    #in1 = Input((50,1024), name='x1')\n",
    "    #x1 = fc_block(in1)\n",
    "    x=x1\n",
    "    #print(x2.shape)\n",
    "    #x = Concatenate(axis=-1)([x1, x2])\n",
    "    #print(x.shape)\n",
    "    #x = Dense(1024)(x)\n",
    "    #x = BatchNormalization()(x)\n",
    "    #x = LeakyReLU()(x)\n",
    "    #x = Dropout(0.2)(x)\n",
    "    #x=fc_block(x)\n",
    "    #print(x)\n",
    "    out = Dense(2000, activation='sigmoid', name='output')(x)\n",
    "    \n",
    "    model = Model(inputs=[in1], outputs=out)\n",
    "    #print(model.summary())\n",
    "    opt = adam_v2.Adam(learning_rate=0.0001)\n",
    "    #opt=gradient_descent_v2.SGD(learning_rate=0.001)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy')\n",
    "    return model\n",
    "def train(X_train,Y_train,X_val,Y_val):\n",
    "    #if not os.path.exists('weights'): os.mkdir('weights')\n",
    "    model = build_mod()\n",
    "    callback = EarlyStopping(monitor='loss', patience=3)\n",
    "    model.fit(X_train[0],Y_train,epochs=100,batch_size=50,\n",
    "                 validation_data = (X_val[0],Y_val),callbacks=[callback])\n",
    "    model.save_weights('weights.h5')\n",
    "    model.load_weights('weights.h5')\n",
    "    y_prd = model.predict({'x1': X_val[0]}, verbose=False, batch_size=50)\n",
    "    g = gap(y_prd, Y_val)\n",
    "    hit=eval.calculate_hit_at_one(y_prd, y_val)\n",
    "    print(\"Hit at one\",hit)\n",
    "    print(\"val GAP\",g)\n",
    "def predict(idx,X_test,Y_test,ylabels):\n",
    "    model = build_mod()\n",
    "    model.load_weights('weights.h5')\n",
    "    ypd = model.predict({'x1': X_test[0]}, verbose=1, batch_size=10)\n",
    "    g = gap(ypd, Y_test)\n",
    "    hit=eval.calculate_hit_at_one(ypd, Y_test)\n",
    "    print(\"Hit at one\",hit)\n",
    "    print(\"int\",g)\n",
    "    #del x1_val, x2_val\n",
    "    with mp.Pool() as pool:\n",
    "        out = pool.map(conv_pred, list(ypd))\n",
    "        out1 = pool.map(another_pred, list(ypd))\n",
    "    df = pd.DataFrame.from_dict({'VideoId': idx,\"True labels\":ylabels,\"Pred labels\":out1, 'Confidence': out})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "50/50 [==============================] - 46s 901ms/step - loss: 19.9022 - val_loss: 27.4925\n",
      "Epoch 2/100\n",
      "50/50 [==============================] - 44s 887ms/step - loss: 15.2852 - val_loss: 24.1111\n",
      "Epoch 3/100\n",
      "50/50 [==============================] - 45s 892ms/step - loss: 12.9610 - val_loss: 22.0214\n",
      "Epoch 4/100\n",
      "50/50 [==============================] - 36s 714ms/step - loss: 12.1187 - val_loss: 20.8249\n",
      "Epoch 5/100\n",
      "50/50 [==============================] - 41s 821ms/step - loss: 12.3000 - val_loss: 21.6275\n",
      "Epoch 6/100\n",
      "50/50 [==============================] - 51s 1s/step - loss: 13.2056 - val_loss: 22.1990\n",
      "Epoch 7/100\n",
      "50/50 [==============================] - 42s 833ms/step - loss: 14.8120 - val_loss: 24.0021\n",
      "Hit at one 0.39285714285714285\n",
      "val GAP 0.32824689326821616\n"
     ]
    }
   ],
   "source": [
    "train(X_train,Y_train,X_val,Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 1s 46ms/step\n",
      "Hit at one 0.3581081081081081\n",
      "int 0.30496295924959493\n"
     ]
    }
   ],
   "source": [
    "df_pooling_audio=predict(idx,X_test,Y_test,ylabels)\n",
    "#Hit@1=0.358\n",
    "#Gap=0.30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_pooling_audio.to_csv(\"data/Frame_level_Pooling_audio.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POOLING ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mod():\n",
    "    in1 = Input((50,128), name='x1')\n",
    "    #print(in1.shape)\n",
    "    x1 = fc_block(in1)\n",
    "    #print(x1.shape)\n",
    "    in2 = Input((50,1024), name='x2')\n",
    "    x2 = fc_block(in2)\n",
    "    #x=x1\n",
    "    #print(x2.shape)\n",
    "    x = Concatenate(axis=-1)([x1, x2])\n",
    "    #print(x.shape)\n",
    "    # x = Dense(1024)(x)\n",
    "    # x = BatchNormalization()(x)\n",
    "    # x = LeakyReLU()(x)\n",
    "    # x = Dropout(0.2)(x)\n",
    "    #x=fc_block(x)\n",
    "    #print(x)\n",
    "    out = Dense(2000, activation='sigmoid', name='output')(x)\n",
    "    \n",
    "    model = Model(inputs=[in1,in2], outputs=out)\n",
    "    #print(model.summary())\n",
    "    #opt = adam_v2.Adam(learning_rate=0.0001)\n",
    "    opt = adam_v2.Adam(learning_rate=0.00001)\n",
    "    #opt=gradient_descent_v2.SGD(learning_rate=0.001)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy')\n",
    "    return model\n",
    "def train(X_train,Y_train,X_val,Y_val):\n",
    "    #if not os.path.exists('weights'): os.mkdir('weights')\n",
    "    model = build_mod()\n",
    "    callback = EarlyStopping(monitor='loss', patience=3,restore_best_weights=True)\n",
    "    model.fit([X_train[0],X_train[1]],Y_train,epochs=100,batch_size=50,\n",
    "                 validation_data = ([X_val[0],X_val[1]],Y_val),callbacks=[callback])\n",
    "    model.save_weights('weights.h5')\n",
    "    model.load_weights('weights.h5')\n",
    "    y_prd = model.predict({'x1': X_val[0],'x2':X_val[1]}, verbose=False, batch_size=50)\n",
    "    g = gap(y_prd, Y_val)\n",
    "    hit=eval.calculate_hit_at_one(y_prd, y_val)\n",
    "    print(\"Hit at one\",hit)\n",
    "    print(\"val GAP\",g)\n",
    "def predict(idx,X_test,Y_test,ylabels):\n",
    "    model = build_mod()\n",
    "    model.load_weights('weights.h5')\n",
    "    ypd = model.predict({'x1': X_test[0],'x2':X_test[1]}, verbose=1, batch_size=10)\n",
    "    g = gap(ypd, Y_test)\n",
    "    hit=eval.calculate_hit_at_one(ypd, Y_test)\n",
    "    print(\"Hit at one\",hit)\n",
    "    print(\"int\",g)\n",
    "    #del x1_val, x2_val\n",
    "    with mp.Pool() as pool:\n",
    "        out = pool.map(conv_pred, list(ypd))\n",
    "        out1 = pool.map(another_pred, list(ypd))\n",
    "    df = pd.DataFrame.from_dict({'VideoId': idx,\"True labels\":ylabels,\"Pred labels\":out1, 'Confidence': out})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "200/200 [==============================] - 347s 2s/step - loss: 20.7740 - val_loss: 18.7114\n",
      "Epoch 2/100\n",
      "200/200 [==============================] - 2480s 12s/step - loss: 17.7959 - val_loss: 16.0977\n",
      "Epoch 3/100\n",
      "200/200 [==============================] - 426s 2s/step - loss: 15.3052 - val_loss: 14.2629\n",
      "Epoch 4/100\n",
      "200/200 [==============================] - 415s 2s/step - loss: 13.4969 - val_loss: 13.0455\n",
      "Epoch 5/100\n",
      "200/200 [==============================] - 797s 4s/step - loss: 12.3347 - val_loss: 12.2834\n",
      "Epoch 6/100\n",
      "200/200 [==============================] - 871s 4s/step - loss: 11.6337 - val_loss: 11.8660\n",
      "Epoch 7/100\n",
      "200/200 [==============================] - 828s 4s/step - loss: 11.2927 - val_loss: 11.7532\n",
      "Epoch 8/100\n",
      "200/200 [==============================] - 896s 4s/step - loss: 11.2286 - val_loss: 11.8730\n",
      "Epoch 9/100\n",
      "200/200 [==============================] - 955s 5s/step - loss: 11.3406 - val_loss: 12.2127\n",
      "Epoch 10/100\n",
      "200/200 [==============================] - 996s 5s/step - loss: 11.6904 - val_loss: 12.7187\n",
      "Epoch 11/100\n",
      "200/200 [==============================] - 941s 5s/step - loss: 12.2458 - val_loss: 13.3912\n",
      "Hit at one 0.6798998569384835\n",
      "val GAP 0.6363714691687508\n"
     ]
    }
   ],
   "source": [
    "train(X_train,Y_train,X_val,Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226/226 [==============================] - 27s 117ms/step\n",
      "Hit at one 0.6748111950244335\n",
      "int 0.6322671281774411\n"
     ]
    }
   ],
   "source": [
    "df_pooling_all=predict(idx,X_test,Y_test,ylabels)\n",
    "#Hit@1=0.67\n",
    "#Gap=0.632"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_pooling_all.to_csv(\"data/Frame_level_Pooling_all.csv\")\n",
    "df_pooling_all.to_pickle(\"data/Frame_level_Pooling_all_1.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIDEO-LEVEL_DENSE LAYER- AUDIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_pooling(frame_data):\n",
    "    # take avaerge across the frames for each video\n",
    "    avg_rgb_by_vid = list(map(\n",
    "        lambda frames: \n",
    "        np.array(frames).mean(axis=0),frame_data))\n",
    "\n",
    "    X= np.array(avg_rgb_by_vid)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_records(data,tp='test'):\n",
    "    #print(batch)\n",
    "    #tfiles = sorted(glob.glob(os.path.join(FOLDER, tp, '*tfrecord')))\n",
    "    # train=['train0093.tfrecord','train0111.tfrecord','train0208.tfrecord','train0274.tfrecord','train076.tfrecord']\n",
    "    # validation=['validate0052.tfrecord','validate0259.tfrecord','validate0267.tfrecord']\n",
    "    # test=['validate3731.tfrecord']\n",
    "    #print('total files in %s %d' % (tp, len(tfiles)))\n",
    "    tfiles=[FOLDER+tp+\"/\"+i for i in data]\n",
    "    ids,aud,rgbs, lbs = [],[],[],[]\n",
    "    for fn in tfiles :\n",
    "        \n",
    "        for example in tf.data.TFRecordDataset(fn):#tf.python_io.tf_record_iterator(fn):\n",
    "            tf_example = tf.train.SequenceExample()#tf.train.Example.FromString(example)\n",
    "            rt=tf_example.ParseFromString(example.numpy())\n",
    "            yss = np.array(tf_example.context.feature[\"labels\"].int64_list.value)\n",
    "            out = np.zeros(2000).astype(np.int8) #number of classes 1000\n",
    "            rgb=[]\n",
    "            audio=[]\n",
    "            ty=len(tf_example.feature_lists.feature_list['rgb'].feature)\n",
    "            #print(\"long\",len(yss))\n",
    "            if np.sum([True for i in yss if i<=1000])==len(yss):\n",
    "                for y in yss:\n",
    "                    out[y] = 1\n",
    "                for k in range(ty):#np.random.randint(0,,100):\n",
    "                    rgb.append(np.array(tf.io.decode_raw(tf_example.feature_lists.feature_list['rgb'].feature[k].bytes_list.value[0],tf.uint8)))\n",
    "                    audio.append(np.array(tf.io.decode_raw(tf_example.feature_lists.feature_list['audio'].feature[k].bytes_list.value[0],tf.uint8)))\n",
    "                ids.append(tf_example.context.feature[\"id\"].bytes_list.value[0].decode(encoding=\"UTF-8\"))\n",
    "                lbs.append(out)\n",
    "\n",
    "                aud.append(np.mean(np.array(audio),axis=0))\n",
    "                rgbs.append(np.mean(np.array(rgb),axis=0))\n",
    "    \n",
    "       \n",
    "    return np.array(ids),np.array(aud), np.array(rgbs), np.array(lbs)\n",
    "def process_labels(data, tp='test'):\n",
    "    #print(batch)\n",
    "    tfiles=[FOLDER+tp+\"/\"+i for i in data]\n",
    "    #print('total files in %s %d' % (tp, len(tfiles)))\n",
    "    ids,ys=[],[]\n",
    "    for fn in tfiles:\n",
    "        \n",
    "        for example in tf.data.TFRecordDataset(fn):\n",
    "            tf_example = tf.train.SequenceExample()\n",
    "            rt=tf_example.ParseFromString(example.numpy())\n",
    "            \n",
    "            yss = np.array(tf_example.context.feature[\"labels\"].int64_list.value)\n",
    "            ty=len(tf_example.feature_lists.feature_list['rgb'].feature)\n",
    "            if np.sum([True for i in yss if i<=1000])==len(yss):\n",
    "                ids.append(tf_example.context.feature[\"id\"].bytes_list.value[0].decode(encoding=\"UTF-8\"))\n",
    "                #print(ids[-1])\n",
    "                ys.append(yss)\n",
    "    return np.array(ys,dtype=object), np.array(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video-level Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(647, 128)\n",
      "(3886, 128)\n",
      "(219, 128)\n"
     ]
    }
   ],
   "source": [
    "_, x1_val, x2_val, y_val = process_records(validation_data,'validation')\n",
    "print(x1_val.shape)\n",
    "_, x1_train, x2_train, y_train=process_records(train_data,'train')\n",
    "print(x1_train.shape)\n",
    "idx, x1_test, x2_test, y_test=process_records(test_data,'test')\n",
    "print(x1_test.shape)\n",
    "ylabels,ids=process_labels(test_data,'test')\n",
    "\n",
    "\n",
    "X_train,Y_train=[x1_train,x2_train],y_train\n",
    "X_val,Y_val=[x1_val,x2_val],y_val\n",
    "idx,X_test,Y_test,ylabels=idx,[x1_test,x2_test],y_test,ylabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_block(x, n=4096, d=0.2):\n",
    "    x = Dense(n)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Dropout(d)(x)\n",
    "    return x\n",
    "def build_mod():\n",
    "    #in1 = Input((128), name='x1')\n",
    "    in1 = Input((128,), name='x1')\n",
    "    print(in1.shape)\n",
    "    #print(in1.shape)\n",
    "    x1 = fc_block(in1)\n",
    "    #print(x1.shape)\n",
    "    #in2 = Input((50,1024), name='x2')\n",
    "    #x2 = fc_block(in2)\n",
    "    x=x1\n",
    "    #print(x2.shape)\n",
    "    #x = Concatenate(axis=-1)([x1, x2])\n",
    "    #print(x.shape)\n",
    "    # x = Dense(1024)(x)\n",
    "    # x = BatchNormalization()(x)\n",
    "    # x = LeakyReLU()(x)\n",
    "    # x = Dropout(0.2)(x)\n",
    "    #x=fc_block(x)\n",
    "    #print(x)\n",
    "    out = Dense(2000, activation='sigmoid', name='output')(x)\n",
    "    \n",
    "    model = Model(inputs=[in1], outputs=out)\n",
    "    #print(model.summary())\n",
    "    opt = adam_v2.Adam(learning_rate=0.000001)\n",
    "    #opt=gradient_descent_v2.SGD(learning_rate=0.001)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy')\n",
    "    return model\n",
    "def train(X_train,Y_train,X_val,Y_val):\n",
    "    #if not os.path.exists('weights'): os.mkdir('weights')\n",
    "    model = build_mod()\n",
    "    callback = EarlyStopping(monitor='loss', patience=3,restore_best_weights=True)\n",
    "    #[X_train[0],X_train[1]]\n",
    "    #[X_val[0],X_val[1]]\n",
    "    model.fit(X_train[0],Y_train,epochs=100,batch_size=50,\n",
    "                 validation_data = (X_val[0],Y_val),callbacks=[callback])\n",
    "    model.save_weights('weights.h5')\n",
    "    model.load_weights('weights.h5')\n",
    "    y_prd = model.predict({'x1': X_val[0]}, verbose=False, batch_size=50)\n",
    "    g = gap(y_prd, Y_val)\n",
    "    hit=eval.calculate_hit_at_one(y_prd, y_val)\n",
    "    print(\"Hit at one\",hit)\n",
    "    print(\"val GAP\",g)\n",
    "\n",
    "def predict(idx,X_test,Y_test,ylabels):\n",
    "    model = build_mod()\n",
    "    model.load_weights('weights.h5')\n",
    "    ypd = model.predict({'x1': X_test[0]}, verbose=1, batch_size=10)\n",
    "    g = gap(ypd, Y_test)\n",
    "    hit=eval.calculate_hit_at_one(ypd, Y_test)\n",
    "    print(\"Hit at one\",hit)\n",
    "    print(\"int\",g)\n",
    "    #del x1_val, x2_val\n",
    "    with mp.Pool() as pool:\n",
    "        out = pool.map(conv_pred, list(ypd))\n",
    "        out1 = pool.map(another_pred, list(ypd))\n",
    "    df = pd.DataFrame.from_dict({'VideoId': idx,\"True labels\":ylabels,\"Pred labels\":out1, 'Confidence': out})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 128)\n",
      "Epoch 1/100\n",
      "78/78 [==============================] - 12s 143ms/step - loss: 21.8669 - val_loss: 24.0770\n",
      "Epoch 2/100\n",
      "78/78 [==============================] - 11s 142ms/step - loss: 21.5962 - val_loss: 21.2297\n",
      "Epoch 3/100\n",
      "78/78 [==============================] - 12s 154ms/step - loss: 21.3730 - val_loss: 20.6992\n",
      "Epoch 4/100\n",
      "78/78 [==============================] - 12s 151ms/step - loss: 21.1309 - val_loss: 20.4538\n",
      "Epoch 5/100\n",
      "78/78 [==============================] - 10s 131ms/step - loss: 20.9080 - val_loss: 20.2518\n",
      "Epoch 6/100\n",
      "78/78 [==============================] - 11s 135ms/step - loss: 20.6669 - val_loss: 20.0607\n",
      "Epoch 7/100\n",
      "78/78 [==============================] - 11s 139ms/step - loss: 20.4429 - val_loss: 19.8698\n",
      "Epoch 8/100\n",
      "78/78 [==============================] - 11s 137ms/step - loss: 20.2343 - val_loss: 19.6872\n",
      "Epoch 9/100\n",
      "78/78 [==============================] - 11s 136ms/step - loss: 20.0228 - val_loss: 19.5051\n",
      "Epoch 10/100\n",
      "78/78 [==============================] - 12s 148ms/step - loss: 19.7926 - val_loss: 19.3272\n",
      "Epoch 11/100\n",
      "78/78 [==============================] - 13s 164ms/step - loss: 19.5958 - val_loss: 19.1527\n",
      "Epoch 12/100\n",
      "78/78 [==============================] - 14s 186ms/step - loss: 19.3978 - val_loss: 18.9827\n",
      "Epoch 13/100\n",
      "78/78 [==============================] - 11s 137ms/step - loss: 19.2014 - val_loss: 18.8156\n",
      "Epoch 14/100\n",
      "78/78 [==============================] - 12s 155ms/step - loss: 18.9984 - val_loss: 18.6527\n",
      "Epoch 15/100\n",
      "78/78 [==============================] - 12s 152ms/step - loss: 18.8030 - val_loss: 18.4932\n",
      "Epoch 16/100\n",
      "78/78 [==============================] - 11s 136ms/step - loss: 18.6269 - val_loss: 18.3351\n",
      "Epoch 17/100\n",
      "78/78 [==============================] - 11s 138ms/step - loss: 18.4546 - val_loss: 18.1810\n",
      "Epoch 18/100\n",
      "78/78 [==============================] - 11s 135ms/step - loss: 18.2750 - val_loss: 18.0286\n",
      "Epoch 19/100\n",
      "78/78 [==============================] - 11s 135ms/step - loss: 18.0895 - val_loss: 17.8801\n",
      "Epoch 20/100\n",
      "78/78 [==============================] - 11s 137ms/step - loss: 17.9097 - val_loss: 17.7324\n",
      "Epoch 21/100\n",
      "78/78 [==============================] - 11s 141ms/step - loss: 17.7419 - val_loss: 17.5896\n",
      "Epoch 22/100\n",
      "78/78 [==============================] - 10s 131ms/step - loss: 17.5658 - val_loss: 17.4478\n",
      "Epoch 23/100\n",
      "78/78 [==============================] - 10s 132ms/step - loss: 17.4113 - val_loss: 17.3075\n",
      "Epoch 24/100\n",
      "78/78 [==============================] - 10s 129ms/step - loss: 17.2523 - val_loss: 17.1734\n",
      "Epoch 25/100\n",
      "78/78 [==============================] - 10s 133ms/step - loss: 17.0604 - val_loss: 17.0389\n",
      "Epoch 26/100\n",
      "78/78 [==============================] - 10s 130ms/step - loss: 16.9159 - val_loss: 16.9056\n",
      "Epoch 27/100\n",
      "78/78 [==============================] - 10s 130ms/step - loss: 16.7731 - val_loss: 16.7796\n",
      "Epoch 28/100\n",
      "78/78 [==============================] - 10s 130ms/step - loss: 16.6065 - val_loss: 16.6505\n",
      "Epoch 29/100\n",
      "78/78 [==============================] - 14s 174ms/step - loss: 16.4568 - val_loss: 16.5223\n",
      "Epoch 30/100\n",
      "78/78 [==============================] - 11s 135ms/step - loss: 16.3075 - val_loss: 16.3997\n",
      "Epoch 31/100\n",
      "78/78 [==============================] - 10s 133ms/step - loss: 16.1550 - val_loss: 16.2809\n",
      "Epoch 32/100\n",
      "78/78 [==============================] - 12s 148ms/step - loss: 16.0259 - val_loss: 16.1630\n",
      "Epoch 33/100\n",
      "78/78 [==============================] - 10s 128ms/step - loss: 15.9101 - val_loss: 16.0489\n",
      "Epoch 34/100\n",
      "78/78 [==============================] - 12s 155ms/step - loss: 15.7286 - val_loss: 15.9364\n",
      "Epoch 35/100\n",
      "78/78 [==============================] - 10s 129ms/step - loss: 15.6294 - val_loss: 15.8236\n",
      "Epoch 36/100\n",
      "78/78 [==============================] - 10s 135ms/step - loss: 15.5021 - val_loss: 15.7146\n",
      "Epoch 37/100\n",
      "78/78 [==============================] - 11s 141ms/step - loss: 15.3452 - val_loss: 15.6092\n",
      "Epoch 38/100\n",
      "78/78 [==============================] - 11s 135ms/step - loss: 15.2354 - val_loss: 15.5032\n",
      "Epoch 39/100\n",
      "78/78 [==============================] - 11s 140ms/step - loss: 15.1020 - val_loss: 15.4012\n",
      "Epoch 40/100\n",
      "78/78 [==============================] - 11s 135ms/step - loss: 15.0049 - val_loss: 15.3041\n",
      "Epoch 41/100\n",
      "78/78 [==============================] - 11s 136ms/step - loss: 14.8805 - val_loss: 15.2049\n",
      "Epoch 42/100\n",
      "78/78 [==============================] - 11s 138ms/step - loss: 14.7649 - val_loss: 15.1091\n",
      "Epoch 43/100\n",
      "78/78 [==============================] - 12s 152ms/step - loss: 14.6477 - val_loss: 15.0175\n",
      "Epoch 44/100\n",
      "78/78 [==============================] - 11s 143ms/step - loss: 14.5324 - val_loss: 14.9279\n",
      "Epoch 45/100\n",
      "78/78 [==============================] - 10s 130ms/step - loss: 14.4325 - val_loss: 14.8420\n",
      "Epoch 46/100\n",
      "78/78 [==============================] - 10s 131ms/step - loss: 14.3309 - val_loss: 14.7577\n",
      "Epoch 47/100\n",
      "78/78 [==============================] - 11s 145ms/step - loss: 14.1990 - val_loss: 14.6730\n",
      "Epoch 48/100\n",
      "78/78 [==============================] - 10s 133ms/step - loss: 14.0929 - val_loss: 14.5974\n",
      "Epoch 49/100\n",
      "78/78 [==============================] - 10s 128ms/step - loss: 14.0294 - val_loss: 14.5181\n",
      "Epoch 50/100\n",
      "78/78 [==============================] - 11s 141ms/step - loss: 13.9458 - val_loss: 14.4401\n",
      "Epoch 51/100\n",
      "78/78 [==============================] - 11s 138ms/step - loss: 13.8295 - val_loss: 14.3694\n",
      "Epoch 52/100\n",
      "78/78 [==============================] - 11s 136ms/step - loss: 13.7677 - val_loss: 14.2992\n",
      "Epoch 53/100\n",
      "78/78 [==============================] - 11s 138ms/step - loss: 13.6600 - val_loss: 14.2293\n",
      "Epoch 54/100\n",
      "78/78 [==============================] - 11s 138ms/step - loss: 13.5886 - val_loss: 14.1643\n",
      "Epoch 55/100\n",
      "78/78 [==============================] - 10s 130ms/step - loss: 13.5009 - val_loss: 14.0986\n",
      "Epoch 56/100\n",
      "78/78 [==============================] - 10s 131ms/step - loss: 13.4183 - val_loss: 14.0395\n",
      "Epoch 57/100\n",
      "78/78 [==============================] - 11s 142ms/step - loss: 13.3580 - val_loss: 13.9814\n",
      "Epoch 58/100\n",
      "78/78 [==============================] - 12s 148ms/step - loss: 13.3109 - val_loss: 13.9243\n",
      "Epoch 59/100\n",
      "78/78 [==============================] - 10s 130ms/step - loss: 13.2151 - val_loss: 13.8676\n",
      "Epoch 60/100\n",
      "78/78 [==============================] - 12s 160ms/step - loss: 13.1766 - val_loss: 13.8149\n",
      "Epoch 61/100\n",
      "78/78 [==============================] - 18s 231ms/step - loss: 13.0628 - val_loss: 13.7635\n",
      "Epoch 62/100\n",
      "78/78 [==============================] - 11s 142ms/step - loss: 13.0212 - val_loss: 13.7153\n",
      "Epoch 63/100\n",
      "78/78 [==============================] - 13s 164ms/step - loss: 12.9526 - val_loss: 13.6654\n",
      "Epoch 64/100\n",
      "78/78 [==============================] - 11s 143ms/step - loss: 12.9247 - val_loss: 13.6186\n",
      "Epoch 65/100\n",
      "78/78 [==============================] - 11s 136ms/step - loss: 12.8326 - val_loss: 13.5747\n",
      "Epoch 66/100\n",
      "78/78 [==============================] - 10s 129ms/step - loss: 12.7887 - val_loss: 13.5330\n",
      "Epoch 67/100\n",
      "78/78 [==============================] - 10s 128ms/step - loss: 12.7340 - val_loss: 13.4939\n",
      "Epoch 68/100\n",
      "78/78 [==============================] - 10s 126ms/step - loss: 12.6973 - val_loss: 13.4532\n",
      "Epoch 69/100\n",
      "78/78 [==============================] - 11s 145ms/step - loss: 12.6630 - val_loss: 13.4176\n",
      "Epoch 70/100\n",
      "78/78 [==============================] - 11s 142ms/step - loss: 12.5997 - val_loss: 13.3830\n",
      "Epoch 71/100\n",
      "78/78 [==============================] - 12s 152ms/step - loss: 12.5530 - val_loss: 13.3486\n",
      "Epoch 72/100\n",
      "78/78 [==============================] - 10s 124ms/step - loss: 12.5090 - val_loss: 13.3167\n",
      "Epoch 73/100\n",
      "78/78 [==============================] - 10s 125ms/step - loss: 12.4539 - val_loss: 13.2849\n",
      "Epoch 74/100\n",
      "78/78 [==============================] - 10s 132ms/step - loss: 12.4326 - val_loss: 13.2548\n",
      "Epoch 75/100\n",
      "78/78 [==============================] - 10s 133ms/step - loss: 12.3680 - val_loss: 13.2273\n",
      "Epoch 76/100\n",
      "78/78 [==============================] - 11s 141ms/step - loss: 12.3394 - val_loss: 13.2013\n",
      "Epoch 77/100\n",
      "78/78 [==============================] - 10s 131ms/step - loss: 12.2905 - val_loss: 13.1756\n",
      "Epoch 78/100\n",
      "78/78 [==============================] - 11s 138ms/step - loss: 12.2786 - val_loss: 13.1515\n",
      "Epoch 79/100\n",
      "78/78 [==============================] - 10s 126ms/step - loss: 12.2239 - val_loss: 13.1279\n",
      "Epoch 80/100\n",
      "78/78 [==============================] - 9s 118ms/step - loss: 12.2299 - val_loss: 13.1055\n",
      "Epoch 81/100\n",
      "78/78 [==============================] - 10s 123ms/step - loss: 12.1660 - val_loss: 13.0850\n",
      "Epoch 82/100\n",
      "78/78 [==============================] - 12s 149ms/step - loss: 12.1511 - val_loss: 13.0651\n",
      "Epoch 83/100\n",
      "78/78 [==============================] - 10s 130ms/step - loss: 12.0871 - val_loss: 13.0456\n",
      "Epoch 84/100\n",
      "78/78 [==============================] - 9s 121ms/step - loss: 12.0622 - val_loss: 13.0281\n",
      "Epoch 85/100\n",
      "78/78 [==============================] - 10s 126ms/step - loss: 12.0384 - val_loss: 13.0117\n",
      "Epoch 86/100\n",
      "78/78 [==============================] - 10s 134ms/step - loss: 12.0353 - val_loss: 12.9961\n",
      "Epoch 87/100\n",
      "78/78 [==============================] - 10s 126ms/step - loss: 11.9926 - val_loss: 12.9807\n",
      "Epoch 88/100\n",
      "78/78 [==============================] - 10s 123ms/step - loss: 11.9830 - val_loss: 12.9676\n",
      "Epoch 89/100\n",
      "78/78 [==============================] - 10s 129ms/step - loss: 11.9454 - val_loss: 12.9543\n",
      "Epoch 90/100\n",
      "78/78 [==============================] - 11s 137ms/step - loss: 11.9575 - val_loss: 12.9421\n",
      "Epoch 91/100\n",
      "78/78 [==============================] - 11s 136ms/step - loss: 11.9374 - val_loss: 12.9310\n",
      "Epoch 92/100\n",
      "78/78 [==============================] - 11s 140ms/step - loss: 11.9053 - val_loss: 12.9207\n",
      "Epoch 93/100\n",
      "78/78 [==============================] - 10s 133ms/step - loss: 11.8644 - val_loss: 12.9113\n",
      "Epoch 94/100\n",
      "78/78 [==============================] - 10s 125ms/step - loss: 11.8631 - val_loss: 12.9026\n",
      "Epoch 95/100\n",
      "78/78 [==============================] - 10s 125ms/step - loss: 11.8344 - val_loss: 12.8933\n",
      "Epoch 96/100\n",
      "78/78 [==============================] - 11s 138ms/step - loss: 11.8281 - val_loss: 12.8880\n",
      "Epoch 97/100\n",
      "78/78 [==============================] - 11s 145ms/step - loss: 11.8008 - val_loss: 12.8795\n",
      "Epoch 98/100\n",
      "78/78 [==============================] - 11s 146ms/step - loss: 11.7827 - val_loss: 12.8757\n",
      "Epoch 99/100\n",
      "78/78 [==============================] - 10s 133ms/step - loss: 11.7496 - val_loss: 12.8703\n",
      "Epoch 100/100\n",
      "78/78 [==============================] - 12s 159ms/step - loss: 11.7429 - val_loss: 12.8646\n",
      "Hit at one 0.46986089644513135\n",
      "val GAP 0.4022778094098029\n"
     ]
    }
   ],
   "source": [
    "train(X_train,Y_train,X_val,Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 128)\n",
      "22/22 [==============================] - 0s 10ms/step\n",
      "Hit at one 0.4611872146118721\n",
      "int 0.40696939756934875\n"
     ]
    }
   ],
   "source": [
    "df_VidDense_audio=predict(idx,X_test,Y_test,ylabels)\n",
    "#Hit@1=0.46\n",
    "#Gap=0.41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_VidDense_audio.to_csv(\"data/Video_level_Dense_audio.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIDEO-LEVEL_DENSE LAYER- RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_block(x, n=4096, d=0.2):\n",
    "    x = Dense(n)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Dropout(d)(x)\n",
    "    return x\n",
    "def build_mod():\n",
    "    #in1 = Input((50,128), name='x1')\n",
    "    #print(in1.shape)\n",
    "    #x1 = fc_block(in1)\n",
    "    #print(x1.shape)\n",
    "    in2 = Input((1024,), name='x2')\n",
    "    x2 = fc_block(in2)\n",
    "    x=x2\n",
    "    #print(x2.shape)\n",
    "    #x = Concatenate(axis=-1)([x1, x2])\n",
    "    #print(x.shape)\n",
    "    # x = Dense(1024)(x)\n",
    "    # x = BatchNormalization()(x)\n",
    "    # x = LeakyReLU()(x)\n",
    "    # x = Dropout(0.2)(x)\n",
    "    #x=fc_block(x)\n",
    "    #print(x)\n",
    "    out = Dense(2000, activation='sigmoid', name='output')(x)\n",
    "    \n",
    "    model = Model(inputs=[in2], outputs=out)\n",
    "    #print(model.summary())\n",
    "    opt = adam_v2.Adam(learning_rate=0.00001)\n",
    "    #opt=gradient_descent_v2.SGD(learning_rate=0.001)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy')\n",
    "    return model\n",
    "def train(X_train,Y_train,X_val,Y_val):\n",
    "    #if not os.path.exists('weights'): os.mkdir('weights')\n",
    "    model = build_mod()\n",
    "    callback = EarlyStopping(monitor='loss', patience=3,restore_best_weights=True)\n",
    "    #[X_train[0],X_train[1]]\n",
    "    #[X_val[0],X_val[1]]\n",
    "    model.fit(X_train[1],Y_train,epochs=100,batch_size=50,\n",
    "                 validation_data = (X_val[1],Y_val),callbacks=[callback])\n",
    "    model.save_weights('weights.h5')\n",
    "    model.load_weights('weights.h5')\n",
    "    y_prd = model.predict({'x2': X_val[1]}, verbose=False, batch_size=50)\n",
    "    g = gap(y_prd, Y_val)\n",
    "    hit=eval.calculate_hit_at_one(y_prd, y_val)\n",
    "    print(\"Hit at one\",hit)\n",
    "    print(\"val GAP\",g)\n",
    "\n",
    "def predict(idx,X_test,Y_test,ylabels):\n",
    "    model = build_mod()\n",
    "    model.load_weights('weights.h5')\n",
    "    ypd = model.predict({'x2': X_test[1]}, verbose=1, batch_size=10)\n",
    "    g = gap(ypd, Y_test)\n",
    "    hit=eval.calculate_hit_at_one(ypd, Y_test)\n",
    "    print(\"Hit at one\",hit)\n",
    "    print(\"int\",g)\n",
    "    #del x1_val, x2_val\n",
    "    with mp.Pool() as pool:\n",
    "        out = pool.map(conv_pred, list(ypd))\n",
    "        out1 = pool.map(another_pred, list(ypd))\n",
    "    df = pd.DataFrame.from_dict({'VideoId': idx,\"True labels\":ylabels,\"Pred labels\":out1, 'Confidence': out})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "78/78 [==============================] - 16s 187ms/step - loss: 20.8629 - val_loss: 23.3939\n",
      "Epoch 2/100\n",
      "78/78 [==============================] - 14s 180ms/step - loss: 18.5631 - val_loss: 17.1703\n",
      "Epoch 3/100\n",
      "78/78 [==============================] - 15s 194ms/step - loss: 16.7047 - val_loss: 15.8512\n",
      "Epoch 4/100\n",
      "78/78 [==============================] - 13s 168ms/step - loss: 15.1702 - val_loss: 14.9623\n",
      "Epoch 5/100\n",
      "78/78 [==============================] - 12s 158ms/step - loss: 13.9246 - val_loss: 14.2324\n",
      "Epoch 6/100\n",
      "78/78 [==============================] - 13s 161ms/step - loss: 12.9156 - val_loss: 13.6287\n",
      "Epoch 7/100\n",
      "78/78 [==============================] - 14s 178ms/step - loss: 12.1720 - val_loss: 13.1803\n",
      "Epoch 8/100\n",
      "78/78 [==============================] - 14s 179ms/step - loss: 11.5716 - val_loss: 12.8429\n",
      "Epoch 9/100\n",
      "78/78 [==============================] - 12s 159ms/step - loss: 11.0958 - val_loss: 12.6066\n",
      "Epoch 10/100\n",
      "78/78 [==============================] - 13s 168ms/step - loss: 10.7528 - val_loss: 12.4379\n",
      "Epoch 11/100\n",
      "78/78 [==============================] - 14s 178ms/step - loss: 10.4570 - val_loss: 12.3247\n",
      "Epoch 12/100\n",
      "78/78 [==============================] - 12s 157ms/step - loss: 10.2408 - val_loss: 12.2577\n",
      "Epoch 13/100\n",
      "78/78 [==============================] - 13s 161ms/step - loss: 10.0769 - val_loss: 12.2313\n",
      "Epoch 14/100\n",
      "78/78 [==============================] - 12s 160ms/step - loss: 9.9098 - val_loss: 12.2440\n",
      "Epoch 15/100\n",
      "78/78 [==============================] - 13s 170ms/step - loss: 9.8167 - val_loss: 12.2894\n",
      "Epoch 16/100\n",
      "78/78 [==============================] - 12s 157ms/step - loss: 9.7327 - val_loss: 12.3384\n",
      "Epoch 17/100\n",
      "78/78 [==============================] - 13s 161ms/step - loss: 9.7042 - val_loss: 12.4266\n",
      "Epoch 18/100\n",
      "78/78 [==============================] - 13s 173ms/step - loss: 9.6761 - val_loss: 12.5496\n",
      "Epoch 19/100\n",
      "78/78 [==============================] - 16s 207ms/step - loss: 9.6714 - val_loss: 12.6810\n",
      "Epoch 20/100\n",
      "78/78 [==============================] - 12s 157ms/step - loss: 9.6949 - val_loss: 12.8406\n",
      "Epoch 21/100\n",
      "78/78 [==============================] - 12s 157ms/step - loss: 9.7198 - val_loss: 12.9911\n",
      "Epoch 22/100\n",
      "78/78 [==============================] - 13s 164ms/step - loss: 9.7924 - val_loss: 13.1394\n",
      "Hit at one 0.5826893353941267\n",
      "val GAP 0.5022035404822881\n"
     ]
    }
   ],
   "source": [
    "train(X_train,Y_train,X_val,Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 14ms/step\n",
      "Hit at one 0.5616438356164384\n",
      "int 0.47673294853661496\n"
     ]
    }
   ],
   "source": [
    "df_Dense_rgb=predict(idx,X_test,Y_test,ylabels)\n",
    "#Hit@1=0.56\n",
    "#Gap=0.48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_Dense_rgb.to_csv(\"data/Video_level_Dense_rgb.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video_level-Dense-ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mod():\n",
    "    in1 = Input((128,), name='x1')\n",
    "    #print(in1.shape)\n",
    "    x1 = fc_block(in1)\n",
    "    #print(x1.shape)\n",
    "    in2 = Input((1024,), name='x2')\n",
    "    x2 = fc_block(in2)\n",
    "    #x=x1\n",
    "    #print(x2.shape)\n",
    "    x = Concatenate(axis=-1)([x1, x2])\n",
    "    #print(x.shape)\n",
    "    # x = Dense(1024)(x)\n",
    "    # x = BatchNormalization()(x)\n",
    "    # x = LeakyReLU()(x)\n",
    "    # x = Dropout(0.2)(x)\n",
    "    x=fc_block(x)\n",
    "    #print(x)\n",
    "    out = Dense(2000, activation='sigmoid', name='output')(x)\n",
    "    \n",
    "    model = Model(inputs=[in1,in2], outputs=out)\n",
    "    #print(model.summary())\n",
    "    opt = adam_v2.Adam(learning_rate=0.00001)\n",
    "    #opt=gradient_descent_v2.SGD(learning_rate=0.001)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy')\n",
    "    return model\n",
    "def train(X_train,Y_train,X_val,Y_val):\n",
    "    #if not os.path.exists('weights'): os.mkdir('weights')\n",
    "    model = build_mod()\n",
    "    callback = EarlyStopping(monitor='loss', patience=3,restore_best_weights=True)\n",
    "    model.fit([X_train[0],X_train[1]],Y_train,epochs=100,batch_size=50,\n",
    "                 validation_data = ([X_val[0],X_val[1]],Y_val),callbacks=[callback])\n",
    "    model.save_weights('weights.h5')\n",
    "    model.load_weights('weights.h5')\n",
    "    y_prd = model.predict({'x1': X_val[0],'x2':X_val[1]}, verbose=False, batch_size=50)\n",
    "    g = gap(y_prd, Y_val)\n",
    "    hit=eval.calculate_hit_at_one(y_prd, y_val)\n",
    "    print(\"Hit at one\",hit)\n",
    "    print(\"val GAP\",g)\n",
    "def predict(idx,X_test,Y_test,ylabels):\n",
    "    model = build_mod()\n",
    "    model.load_weights('weights.h5')\n",
    "    ypd = model.predict({'x1': X_test[0],'x2':X_test[1]}, verbose=1, batch_size=10)\n",
    "    g = gap(ypd, Y_test)\n",
    "    hit=eval.calculate_hit_at_one(ypd, Y_test)\n",
    "    print(\"Hit at one\",hit)\n",
    "    print(\"int\",g)\n",
    "    #del x1_val, x2_val\n",
    "    with mp.Pool() as pool:\n",
    "        out = pool.map(conv_pred, list(ypd))\n",
    "        out1 = pool.map(another_pred, list(ypd))\n",
    "    df = pd.DataFrame.from_dict({'VideoId': idx,\"True labels\":ylabels,\"Pred labels\":out1, 'Confidence': out})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "78/78 [==============================] - 69s 863ms/step - loss: 19.3420 - val_loss: 19.5680\n",
      "Epoch 2/100\n",
      "78/78 [==============================] - 78s 1s/step - loss: 14.6078 - val_loss: 14.4708\n",
      "Epoch 3/100\n",
      "78/78 [==============================] - 56s 714ms/step - loss: 12.4102 - val_loss: 13.0989\n",
      "Epoch 4/100\n",
      "78/78 [==============================] - 67s 864ms/step - loss: 11.5882 - val_loss: 12.6318\n",
      "Epoch 5/100\n",
      "78/78 [==============================] - 46s 591ms/step - loss: 11.2809 - val_loss: 12.6349\n",
      "Epoch 6/100\n",
      "78/78 [==============================] - 70s 895ms/step - loss: 11.3242 - val_loss: 12.9591\n",
      "Epoch 7/100\n",
      "78/78 [==============================] - 35s 447ms/step - loss: 11.5038 - val_loss: 13.4656\n",
      "Epoch 8/100\n",
      "78/78 [==============================] - 42s 542ms/step - loss: 11.8891 - val_loss: 14.0555\n",
      "Hit at one 0.6089644513137558\n",
      "val GAP 0.5416108063832797\n"
     ]
    }
   ],
   "source": [
    "train(X_train,Y_train,X_val,Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 1s 57ms/step\n",
      "Hit at one 0.5844748858447488\n",
      "int 0.5288556928281277\n"
     ]
    }
   ],
   "source": [
    "df_VidDense_all=predict(idx,X_test,Y_test,ylabels)\n",
    "#Hit@1=0.58\n",
    "#Gap=0.53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_VidDense_all.to_csv(\"data/Video_level_Dense_all.csv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5e9f11f49cd656f291f5777c461e4288f6343aeff66d80fd968f2fc2025425e8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
